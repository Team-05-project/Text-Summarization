{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f61dc35-2ae3-45ea-b02d-3ef4c9b27cf3",
   "metadata": {},
   "source": [
    "# INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f834d26-3ddf-4a30-950a-0cf7a7c6672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                path filename  category  \\\n",
      "0  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  001.txt  business   \n",
      "1  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  002.txt  business   \n",
      "2  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  003.txt  business   \n",
      "3  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  004.txt  business   \n",
      "4  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  005.txt  business   \n",
      "\n",
      "  article_or_summary  \n",
      "0      News Articles  \n",
      "1      News Articles  \n",
      "2      News Articles  \n",
      "3      News Articles  \n",
      "4      News Articles  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "path_ = []\n",
    "filename_ = []\n",
    "category_ = []\n",
    "article_or_summary_ = []\n",
    "\n",
    "# Define the root folder (replace with your path)\n",
    "root_folder = r\"C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC News Summary\"\n",
    "\n",
    "# Traverse the folder structure\n",
    "for dirname, _, filenames in os.walk(root_folder):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.txt'):  # Only process .txt files\n",
    "            # Full path to the file\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            path_.append(full_path)\n",
    "            filename_.append(filename)\n",
    "            \n",
    "            # Normalize path for consistent splitting\n",
    "            normalized_path = os.path.normpath(dirname)\n",
    "            \n",
    "            # Extract article_or_summary (parent folder of category)\n",
    "            article_or_summary_.append(os.path.split(os.path.dirname(normalized_path))[-1])\n",
    "            \n",
    "            # Extract category (current folder)\n",
    "            category_.append(os.path.split(normalized_path)[-1])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"path\": path_,\n",
    "        \"filename\": filename_,\n",
    "        \"category\": category_,\n",
    "        \"article_or_summary\": article_or_summary_,\n",
    "    },\n",
    "    columns=[\"path\", \"filename\", \"category\", \"article_or_summary\"]\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f9486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "      <th>article_or_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>News Articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>News Articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>News Articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>News Articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>business</td>\n",
       "      <td>News Articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4445</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Summaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4450 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path filename  category  \\\n",
       "0     C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  001.txt  business   \n",
       "1     C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  002.txt  business   \n",
       "2     C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  003.txt  business   \n",
       "3     C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  004.txt  business   \n",
       "4     C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  005.txt  business   \n",
       "...                                                 ...      ...       ...   \n",
       "4445  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  397.txt      tech   \n",
       "4446  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  398.txt      tech   \n",
       "4447  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  399.txt      tech   \n",
       "4448  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  400.txt      tech   \n",
       "4449  C:/Users/Lavanya Gurram/OneDrive/Desktop/BBC N...  401.txt      tech   \n",
       "\n",
       "     article_or_summary  \n",
       "0         News Articles  \n",
       "1         News Articles  \n",
       "2         News Articles  \n",
       "3         News Articles  \n",
       "4         News Articles  \n",
       "...                 ...  \n",
       "4445          Summaries  \n",
       "4446          Summaries  \n",
       "4447          Summaries  \n",
       "4448          Summaries  \n",
       "4449          Summaries  \n",
       "\n",
       "[4450 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"path\":path_, \"filename\":filename_, \"category\":category_, \"article_or_summary\":article_or_summary_}, columns=[\"path\", \"filename\", \"category\", \"article_or_summary\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad447e5c-874f-4c30-8793-2ea39888d83f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ade87e-570c-4087-9ed7-422ad9113bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (5.24.1)\n",
      "Requirement already satisfied: cufflinks in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from plotly) (23.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19.2 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (2.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (1.16.0)\n",
      "Requirement already satisfied: colorlover>=0.2.1 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (0.3.0)\n",
      "Requirement already satisfied: setuptools>=34.4.1 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (65.5.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (8.14.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from cufflinks) (8.1.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipython>=5.3.0->cufflinks) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipywidgets>=7.0.0->cufflinks) (0.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipywidgets>=7.0.0->cufflinks) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from ipywidgets>=7.0.0->cufflinks) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from pandas>=0.19.2->cufflinks) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from pandas>=0.19.2->cufflinks) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from pandas>=0.19.2->cufflinks) (2024.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->cufflinks) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->cufflinks) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from stack-data->ipython>=5.3.0->cufflinks) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from stack-data->ipython>=5.3.0->cufflinks) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lavanya gurram\\iba_ioapdata\\lavanyaprg\\lib\\site-packages (from stack-data->ipython>=5.3.0->cufflinks) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install plotly cufflinks\n",
    "\n",
    "# Import necessary libraries\n",
    "import plotly.express as px  # plotly.express is the correct import for plotly_express\n",
    "import cufflinks as cf\n",
    "\n",
    "# Enable offline mode for cufflinks\n",
    "cf.go_offline()\n",
    "\n",
    "# Optional: Set a theme for visualizations (if desired)\n",
    "cf.set_config_file(theme=\"pearl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa30f9f-088d-4cab-8b92-6b4b0013be60",
   "metadata": {},
   "source": [
    "## Distribution of Number of Articles in Each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ct = Counter(df[df['article_or_summary']==\"News Articles\"][\"category\"])\n",
    "pd.DataFrame({\"category\":ct.keys(), \"value\":ct.values()}).iplot(kind='bar', x='category', y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b103043-f895-4fa1-9a99-12b9a68ffebb",
   "metadata": {},
   "source": [
    "## Distribution of Category and its Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d068d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"category\":ct.keys(), \"value\":ct.values()}).iplot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e174d11-f67a-4361-b66a-3abf12ce2935",
   "metadata": {},
   "source": [
    "## Distribution Size of Each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc314be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"category\":ct.keys(), \"value\":ct.values()}).iplot(kind='bubble', x='category', y='value', size='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020e344-4e55-4695-b9eb-d0b2c3dd9759",
   "metadata": {},
   "source": [
    "## Coverage Ratio of Each Category¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f477529",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"category\":ct.keys(), \"value\":ct.values()}).iplot(kind='pie', labels=\"category\", values='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1919a5-36bd-4bd4-8a74-fdda9ae14601",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538d19a-bb45-456c-bfd8-a6f0a741671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd34a4-bba5-43ba-bee0-717396f3ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def read_article(text):        \n",
    "    sentences =[]        \n",
    "    sentences = sent_tokenize(text)    \n",
    "    for sentence in sentences:        \n",
    "        sentence.replace(\"[^a-zA-Z0-9]\",\" \")     \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092198a-5b40-4835-951d-72eac83863cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = df[df['article_or_summary']=='News Articles'].iloc[0]['path']\n",
    "with open(file_path, \"r\") as f:\n",
    "    article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200665ad-ed34-4850-8e6c-20fda68d0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tok = read_article(article)\n",
    "sent_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e567b-18df-45cc-b25a-b5d18aee78c1",
   "metadata": {},
   "source": [
    "# Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e1d33-d6ca-4228-8838-a903dbc17af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize an empty list to store corrected sentences\n",
    "mod_sent = []\n",
    "\n",
    "# Iterate over each sentence in the tokenized sentences\n",
    "for tok in sent_tok:\n",
    "    blob_obj = TextBlob(tok)  # Create a TextBlob object for the sentence\n",
    "    correct_sent = str(blob_obj.correct())  # Correct the sentence using TextBlob\n",
    "    print(f\"\\033[94mOriginal Token : {tok}\\033[0m\")\n",
    "    print(f\"\\033[92mCorrected Token: {correct_sent}\\033[0m\")\n",
    "    mod_sent.append(correct_sent)  # Add the corrected sentence to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806aa406-90b5-4827-a380-20e991d58d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(mod_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859fd73-1233-4f6f-9809-084c5c5c51a1",
   "metadata": {},
   "source": [
    "# Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e50f33-110e-4281-92cf-6339a3cbdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Load the Universal Sentence Encoder model from TensorFlow Hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def sentence_similarity(sent1: str, sent2: str, embed) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two sentences.\n",
    "\n",
    "    Args:\n",
    "        sent1 (str): First sentence.\n",
    "        sent2 (str): Second sentence.\n",
    "        embed: Pre-loaded Universal Sentence Encoder model.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine distance between the two sentence embeddings.\n",
    "    \"\"\"\n",
    "    # Get the embeddings for the sentences\n",
    "    embedding1 = embed([sent1])[0].numpy()\n",
    "    embedding2 = embed([sent2])[0].numpy()\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "    \n",
    "    # Return cosine distance (1 - cosine similarity)\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95632-de19-450e-a303-e6882e90cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sentences and their similarity score\n",
    "sentence1 = mod_sent[0]\n",
    "sentence2 = mod_sent[1]\n",
    "similarity_score = sentence_similarity(sentence1, sentence2, embed)\n",
    "\n",
    "print(f\"\\033[92mSentence 1        : {sentence1}\\033[0m\")\n",
    "print(f\"\\033[92mSentence 2        : {sentence2}\\033[0m\")\n",
    "print(f\"\\033[92mSimilarity Score  : {similarity_score:.4f}\\033[0m\")  # Rounded to 4 decimal places for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3237e9-363d-4a9b-992b-f7a3990f0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_similarity_matrix(sentences: list, embeds) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a similarity matrix for a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): A list of sentences.\n",
    "        embeds: Pre-loaded Universal Sentence Encoder model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A similarity matrix where each entry (i, j) represents \n",
    "                    the similarity score between sentences i and j.\n",
    "    \"\"\"\n",
    "    num_sentences = len(sentences)\n",
    "    similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "    for idx1 in range(num_sentences):\n",
    "        for idx2 in range(idx1 + 1, num_sentences):  # Compute only upper triangle\n",
    "            similarity = sentence_similarity(sentences[idx1], sentences[idx2], embeds)\n",
    "            similarity_matrix[idx1][idx2] = similarity\n",
    "            similarity_matrix[idx2][idx1] = similarity  # Symmetric assignment\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c1f0c-f784-480b-9a2d-cbb02e7c823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the similarity matrix using the corrected sentences and the embedding model\n",
    "similarity_matrix = build_similarity_matrix(mod_sent, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e842627-d775-43ac-9a80-4670a9064afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import from_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Example similarity matrix\n",
    "similarity_matrix = np.random.rand(10, 10)  # Replace with your actual similarity matrix\n",
    "\n",
    "# Creating the graph\n",
    "g = nx.Graph()\n",
    "\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        if similarity_matrix[i][j] >= 0.9:\n",
    "            g.add_edge(i, j)\n",
    "\n",
    "# Tooltip and plot setup\n",
    "HOVER_TOOLTIPS = [(\"sent_tok\", \"@index\")]\n",
    "plot = figure(tooltips=HOVER_TOOLTIPS, tools=\"pan,wheel_zoom,save,reset\",\n",
    "              active_scroll='wheel_zoom',\n",
    "              x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1))\n",
    "\n",
    "# Creating the network graph\n",
    "network_graph = from_networkx(g, nx.spring_layout, scale=7, center=(0, 0))\n",
    "network_graph.node_renderer.glyph = Circle(radius=0.1, fill_color='green')  # Changed 'size' to 'radius'\n",
    "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
    "\n",
    "# Adding the network graph to the plot\n",
    "plot.renderers.append(network_graph)\n",
    "\n",
    "# Display the plot\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba18dd-0a01-4c45-87df-5a687c24a64a",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85229dcd-b2e1-4416-be12-3a23006f484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the file path for 'Summaries'\n",
    "file_path_summary = df.loc[df['article_or_summary'] == 'Summaries', 'path'].iloc[0]\n",
    "\n",
    "# Read the file content\n",
    "try:\n",
    "    with open(file_path_summary, \"r\") as file:\n",
    "        actual_summary = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at path {file_path_summary}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce6a4a-4a95-4c3f-ac95-179012607af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text: str, top_n: int, embeds) -> str:\n",
    "    sentences = read_article(text)\n",
    "\n",
    "    # Handle cases with fewer sentences than top_n\n",
    "    if len(sentences) < top_n:\n",
    "        top_n = len(sentences)\n",
    "\n",
    "    if not sentences:\n",
    "        return \"No content to summarize.\"\n",
    "\n",
    "    sentence_similarity_matrix = build_similarity_matrix(sentences, embeds)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentences = sorted(\n",
    "        ((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True\n",
    "    )\n",
    "\n",
    "    summarize_text = [ranked_sentences[i][1] for i in range(top_n)]\n",
    "    return \" \".join(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9049c-c30b-41ab-94df-97c0250d4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all corrected sentences into the original text\n",
    "original_text = \" \".join(mod_sent)\n",
    "\n",
    "# Generate a summary of the original text\n",
    "summarized_text = generate_summary(text=original_text, top_n=5, embeds=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fd773-7f45-4ad5-b628-7f6dc7099618",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d7a3e-7e62-4774-8e3e-837001975bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96fe69-0248-43bd-a2a2-52bed41f01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a2885-c72f-43d8-99c1-59a36904bda5",
   "metadata": {},
   "source": [
    "# Validation \n",
    "There are Multiple ways we can compary Two sentences to compute accuracy\n",
    "\r\n",
    "N-Grams/Bleu Score : Mostly used in Translation\r\n",
    "Similarity Score for Computing similarity from two sentences : Used mostly for Summary comparision or similar word/sentence Search.\r\n",
    "In Our case 2nd option is best but will implement both Cases and see the difference of scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534b6b1-8aac-4bf6-a946-1d9bd1974e8d",
   "metadata": {},
   "source": [
    "# N-Grams/Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd0d98-7d12-40c6-b23c-bb6c2b3a55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = summarized_text\n",
    "reference = actual_summary\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(f\"BLEUscore : {BLEUscore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093ca7c-1ead-49fb-ba92-fb39ce56f821",
   "metadata": {},
   "source": [
    "# Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025219e-04b1-4988-9d7c-5b7131db7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1,sent2,embed):  \n",
    "    A = embed([sent1])[0]\n",
    "    B = embed([sent2])[0]\n",
    "    return 1 - (np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeac961-50b9-4a30-9029-333008b94df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Senetence Similarity Score : {sentence_similarity(summarized_text, actual_summary, embed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac7800-89d6-435d-b689-a505953403e1",
   "metadata": {},
   "source": [
    "# Summarization With Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebd453-a37d-43f3-95d3-289c0fe523b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  sumy\n",
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8347f-dd55-4c56-bea3-35de986cb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_string(original_text,Tokenizer(\"english\"))\n",
    "\n",
    "summarizer = LexRankSummarizer()\n",
    "#Summarize the document with 2 sentences\n",
    "summary = summarizer(parser.document, 5)\n",
    "\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef92cb5-eca1-471f-ae5c-0960e510635b",
   "metadata": {},
   "source": [
    "# CONCLUSION \n",
    "As we approach the end of implementation Summarizing what we have done and achieved thus far:\r",
    "-> \n",
    "We gathered BBC articles and their summaries for the purpose of reference and comparison.-> \r\n",
    "We gathered a variety of methodologies and strategies for text summarization, including extractive and abstractive methods-> .\r\n",
    "We delved further into extraction procedure-> s.\r\n",
    "I picked up the Graph Implementation technique for Extractive Text Summarization. Six articles were converted to Senetence Toke-> ns.\r\n",
    "A similarity matrix was computed for graph format-> ion.\r\n",
    "The Page rank method was used to rank sentence tokens, and the top N were chosen to reflect the sum-> mary.\r\n",
    "For the validation portion, we utilized both BleuScore and Similarity Score and discovered that Bleu cannot be used in our scenario and that Similarity Score is considerably more dependable.\r\n",
    "Summarizing using Python is a :\n",
    "\r\n",
    "This section allows us to compare and contrast our solution with some of the most popular Python tools available.\r\n",
    "Summy is a simple library and command-line application that extracts summary information from HTML pages or plain text. The program also includes a simple assessment mechanism for text summaries. The implemented summarizing techniques are given in the documentation.ailable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
